{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Convolutional neural network\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "inputs = torch.randn(1,1,32,32, requires_grad=True)\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "optimizer.zero_grad()\n",
    "out = net(inputs)\n",
    "target = torch.arange(1,11)\n",
    "target = target.view(1,-1)\n",
    "loss = criterion(out, target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9189, -1.2199, -1.1849],\n",
      "        [-0.8869, -1.3236, -1.1335],\n",
      "        [-0.8511, -1.2644, -1.2356],\n",
      "        [-0.8072, -1.3592, -1.2140]])\n",
      "train start\n",
      "epoch:0, loss:1.2430949211120605\n",
      "epoch:50, loss:0.6022355556488037\n",
      "epoch:100, loss:0.08683717995882034\n",
      "epoch:150, loss:0.027351120486855507\n",
      "epoch:200, loss:0.014909886755049229\n",
      "epoch:250, loss:0.009966205805540085\n",
      "train done\n",
      "tensor([[-7.0281, -0.0024, -6.4868],\n",
      "        [-4.9816, -6.3414, -0.0087],\n",
      "        [-0.0355, -3.6941, -4.6025],\n",
      "        [-5.4978, -0.0047, -7.3821]])\n",
      "tensor([ 1,  2,  0,  1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "    POS tagging \n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def preparedata(seq, to_idx):\n",
    "    if to_idx is word2idx:\n",
    "        idxs = [to_idx[w.lower()] for w in seq]\n",
    "    else:\n",
    "        idxs = [to_idx[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "train_data = [\n",
    "    (\"the dog eat an apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"cat like fish\".split(), [\"NN\", \"V\", \"NN\"]),\n",
    "]\n",
    "\n",
    "word2idx = set()\n",
    "for seq, tags in train_data:\n",
    "    for word in seq:\n",
    "        word = word.lower()\n",
    "        if word not in word2idx:\n",
    "            word2idx.add(word)\n",
    "word2idx = {word: idx for idx,word in enumerate(sorted(word2idx))}\n",
    "tag2idx = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# hyperparamters\n",
    "HIDDEN_DIM = 16\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "\n",
    "class MyLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, Embedding_dim, Hidden_dim, vocab_size, target_size):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = Hidden_dim\n",
    "        self.embeds = nn.Embedding(vocab_size, Embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=Embedding_dim, hidden_size=self.hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(self.hidden_dim, target_size)\n",
    "        self.hidden = self.init_hidden()\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeds(sentence)\n",
    "        lstm_outputs, self.hidden = self.lstm(embeds.view(len(sentence),1,-1), self.hidden)\n",
    "        tag_space = self.hidden2tag(lstm_outputs.view(len(sentence),-1))\n",
    "        tag_score = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_score\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1,1,self.hidden_dim),\n",
    "                torch.zeros(1,1,self.hidden_dim))\n",
    "    \n",
    "model = MyLSTM(EMBEDDING_DIM, HIDDEN_DIM, len(word2idx), len(tag2idx))\n",
    "\n",
    "loss_func = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = preparedata(train_data[1][0], word2idx)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "    \n",
    "print('train start')\n",
    "for i in range(300):\n",
    "    for sentence, tags in train_data:\n",
    "        model.zero_grad()\n",
    "        model.hidden = model.init_hidden()\n",
    "        \n",
    "        inputs = preparedata(sentence, word2idx)\n",
    "        targets = preparedata(tags, tag2idx)\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if i % 50 == 0:\n",
    "        print(\"epoch:{}, loss:{}\".format(i, loss.item()))\n",
    "print('train done')\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = preparedata(train_data[1][0], word2idx)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "    print(np.argmax(np.exp(tag_scores), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 688\n",
      "古诗样本数: 63\n",
      "是否强制训练(Y/N):N\n",
      "tensor([[ 5]])\n",
      "tensor([[ 596]])\n",
      "tensor([[ 166]])\n",
      "tensor([[ 650]])\n",
      "tensor([[ 408]])\n",
      "tensor([[ 166]])\n",
      "tensor([[ 687]])\n",
      "tensor([[ 165]])\n",
      "tensor([[ 379]])\n",
      "tensor([[ 12]])\n",
      "tensor([[ 36]])\n",
      "tensor([[ 455]])\n",
      "tensor([[ 1]])\n",
      "tensor([[ 349]])\n",
      "tensor([[ 524]])\n",
      "丛上连天霜满天，夜泊东人相。桃花\n",
      "tensor([[ 369]])\n",
      "tensor([[ 7]])\n",
      "tensor([[ 346]])\n",
      "tensor([[ 671]])\n",
      "tensor([[ 432]])\n",
      "tensor([[ 356]])\n",
      "tensor([[ 687]])\n",
      "tensor([[ 257]])\n",
      "tensor([[ 559]])\n",
      "tensor([[ 465]])\n",
      "tensor([[ 329]])\n",
      "tensor([[ 165]])\n",
      "tensor([[ 239]])\n",
      "tensor([[ 524]])\n",
      "tensor([[ 1]])\n",
      "流水不树鸣玉楼，忽见离有夜庭花。\n",
      "tensor([[ 36]])\n",
      "tensor([[ 558]])\n",
      "tensor([[ 590]])\n",
      "tensor([[ 682]])\n",
      "tensor([[ 677]])\n",
      "tensor([[ 356]])\n",
      "tensor([[ 687]])\n",
      "tensor([[ 420]])\n",
      "tensor([[ 524]])\n",
      "tensor([[ 4]])\n",
      "tensor([[ 328]])\n",
      "tensor([[ 6]])\n",
      "tensor([[ 282]])\n",
      "tensor([[ 220]])\n",
      "tensor([[ 1]])\n",
      "年人西辞黄鹤楼，烟花三月下扬州。\n",
      "tensor([[ 494]])\n",
      "tensor([[ 455]])\n",
      "tensor([[ 484]])\n",
      "tensor([[ 2]])\n",
      "tensor([[ 551]])\n",
      "tensor([[ 419]])\n",
      "tensor([[ 687]])\n",
      "tensor([[ 588]])\n",
      "tensor([[ 420]])\n",
      "tensor([[ 203]])\n",
      "tensor([[ 120]])\n",
      "tensor([[ 664]])\n",
      "tensor([[ 5]])\n",
      "tensor([[ 1]])\n",
      "tensor([[ 0]])\n",
      "载经相管一蜡烛，轻烟小合马上。\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    poetry-gen\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "\n",
    "word_dict = set()\n",
    "\n",
    "raw_data = [\n",
    "    '雨过横塘水满堤，乱山高下路东西。一番桃李花开尽，惟有青青草色齐。',\n",
    "    '京口瓜洲一水间，钟山只隔数重山。春风又绿江南岸，明月何时照我还。',\n",
    "    '茅檐长扫静无苔，花木成畦手自栽。一水护田将绿绕，两山排闼送青来。',\n",
    "    '李白乘舟将欲行，忽闻岸上踏歌声。桃花潭水深千尺，不及汪伦送我情。',\n",
    "    '朝辞白帝彩云间，千里江陵一日还。两岸猿声啼不住，轻舟已过万重山。',\n",
    "    '天门中断楚江开，碧水东流至此回。两岸青山相对出，孤帆一片日边来。',\n",
    "    '峨眉山月半轮秋，影入平羌江水流。夜发清溪向三峡，思君不见下渝州。',\n",
    "    '故人西辞黄鹤楼，烟花三月下扬州。孤帆远影碧空尽，唯见长江天际流。',\n",
    "    '杨花落尽子规啼，闻道龙标过五溪。我寄愁心与明月，随君直到夜郎西。',\n",
    "    '百战沙场碎铁衣，城南已合数重围。突营射杀呼延将，独领残兵千骑归。',\n",
    "    '越王句践破吴归，义士还乡尽锦衣。宫女如花满春殿，只今惟有鹧鸪飞。',\n",
    "    '问余何意栖碧山，笑而不答心自闲。桃花流水窅然去，别有天地非人间。',\n",
    "    '两个黄鹂鸣翠柳，一行白露上青天。窗含西岭千秋雪，门泊东吴万里船。',\n",
    "    '三万里河东人海，五千仍岳上摩天。遗民泪尽胡尘里，南望王师又一年。',\n",
    "    '曾经沧海难为水，除却巫山不是云。取次花丛懒回顾，半缘修道半缘君。',\n",
    "    '去年今日此门中，人面桃花相映红。人面不知何处去，桃花依旧笑春风。',\n",
    "    '多情却似总无情，唯觉樽前笑不成。蜡烛有心还惜别，替人垂泪到天明。',\n",
    "    '青山隐隐水迢迢，秋尽江南草未凋。二十四桥明月夜，玉人何处教吹箫。',\n",
    "    '公子王孙逐后尘，绿珠垂泪滴罗巾。侯门一入深似海，从此萧郎是路人。',\n",
    "    '落魄江湖载酒行，楚腰纤细掌中轻。十年一觉扬州梦，赢得青楼薄幸名。',\n",
    "    '岁岁金河复玉关，朝朝马策与刀环。三春白雪归青冢，万里黄河绕黑山。',\n",
    "    '独在异乡为异客，每逢佳节倍思亲。遥知兄弟登高处，遍插茱萸少一人。',\n",
    "    '日照香炉生紫烟，遥看瀑布挂前川。飞流直下三千尺，疑是银河落九天。',\n",
    "    '寒雨连江夜入吴，平明送客楚山孤。洛阳亲友如相问，一片冰心在玉壶。',\n",
    "    '闺中少妇不知愁，春日凝妆上翠楼。忽见陌头杨柳色，悔教夫婿觅封侯。',\n",
    "    '葡萄美酒夜光杯，欲饮琵琶马上催。醉卧沙场君莫笑，古来征战几人回。',\n",
    "    '独怜幽草涧边生，上有黄鹂深树鸣。春潮带雨晚来急，野渡无人舟自横。',\n",
    "    '春城无处不飞花，寒食东风御柳斜。日暮汉宫传蜡烛，轻烟散入五侯家。',\n",
    "    '玉楼天半起笙歌，风送宫嫔笑语和。月殿影开闻夜漏，水晶帘卷近秋河。',\n",
    "    '朱雀桥边野草花，乌衣巷口夕阳斜。旧时王谢堂前燕，飞入寻常百姓家。',\n",
    "    '新妆宜面下朱楼，深锁春光一院愁。行到中庭数花朵，蜻蜓飞上玉搔头。',\n",
    "    '寂寂花时闭院门，美人相并立琼轩。含情欲说宫中事，鹦鹉前头不敢言。',\n",
    "    '折戟沉沙铁未销，自将磨洗认前朝。东风不与周郎便，铜雀春深锁二乔。',\n",
    "    '烟笼寒水月笼沙，夜泊秦淮近酒家。商女不知亡国恨，隔江犹唱后庭花。',\n",
    "    '落魄江湖载酒行，楚腰纤细掌中轻。十年一觉扬州梦，赢得青楼薄幸名。',\n",
    "    '银烛秋光冷画屏，轻罗小扇扑流萤。天阶夜色凉如水，坐看牵牛织女星。',\n",
    "    '娉娉袅袅十三余，豆蔻梢头二月初。春风十里扬州路，卷上珠帘总不如。',\n",
    "    '君问归期未有期，巴山夜雨涨秋池。何当共剪西窗烛，却话巴山夜雨时。',\n",
    "    '嵩云秦树久离居，双鲤迢迢一纸书。休问梁园旧宾客，茂陵秋雨病相如。',\n",
    "    '云母屏风烛影深，长河渐落晓星沉。嫦娥应悔偷灵药，碧海青天夜夜心。',\n",
    "    '江雨霏霏江草齐，六朝如梦鸟空啼。无情最是台城柳，依旧烟笼十里堤。',\n",
    "    '别梦依依到谢家，小廊回合曲阑斜。多情只有春庭月，犹为离人照落花。',\n",
    "    '近寒食雨草萋萋，著麦苗风柳映堤。等是有家归未得，杜鹃休向耳边啼。',\n",
    "    '别梦依依到谢家，小廊回合曲阑斜。多情只有春庭月，犹为离人照落花。',\n",
    "    '岁暮阴阳催短景，天涯霜雪霁寒霄。五更鼓角声悲壮，三峡星河影动摇。',\n",
    "    '燕台一去客心惊，箫鼓喧喧汉将营。万里寒光生积雪，三边曙色动危旌。',\n",
    "    '沙场烽火侵胡月，海畔云山拥蓟城。少小虽非投笔吏，论功还欲请长缨。',\n",
    "    '独怜幽草涧边生，上有黄鹂深树鸣。春潮带雨晚来急，野渡无人舟自横。',\n",
    "    '月落乌啼霜满天，江枫渔火对愁眠。姑苏城外寒山寺，夜半钟声到客船。',\n",
    "    '回乐峰前沙似雪，受降城外月如霜。不知何处吹芦管，一夜征人尽望乡。',\n",
    "    '红树青山日欲斜，长郊草色绿无涯。游人不管春将老，来往亭前踏落花。',\n",
    "    '夜凉吹笛千山月，路暗迷人百种花。棋罢不知人换世，酒阑无奈客思家。',\n",
    "    '胜败兵家事不期，包羞忍耻是男儿。江东子弟多才俊，卷土重来未可知。',\n",
    "    '凤凰台上凤凰游，凤去台空江自流。吴宫花草埋幽径，晋代衣冠成古丘。',\n",
    "    '三山半落青天外，二水中分白鹭洲。总为浮云能蔽日，长安不见使人愁。',\n",
    "    '舍南舍北皆春水，但见群鸥日日来。花径不曾缘客扫，蓬门今始为君开。',\n",
    "    '西山白雪三城戍，南浦清江万里桥。海内风尘诸弟隔，天涯涕泪一身遥。',\n",
    "    '剑外忽传收蓟北，初闻涕泪满衣裳。却看妻子愁何在，漫卷诗书喜欲狂。',\n",
    "    '白日放歌须纵酒，青春作伴好还乡。即从巴峡穿巫峡，便下襄阳向洛阳。',\n",
    "    '风急天高猿啸哀，渚清沙白鸟飞回。无边落木萧萧下，不尽长江滚滚来。',\n",
    "    '万里悲秋常作客，百年多病独登台。艰难苦恨繁霜鬓，潦倒新停浊酒杯。',\n",
    "    '汀洲无浪复无烟，楚客相思益渺然。汉口夕阳斜渡鸟，洞庭秋水远连天。',\n",
    "    '孤城背岭寒吹角，独戍临江夜泊船。贾谊上书忧汉室，长沙谪去古今怜。',\n",
    "]\n",
    "\n",
    "first_char = set()\n",
    "for sentence in raw_data:\n",
    "    first_char.add(sentence[0])\n",
    "    first_char.add(sentence[8])\n",
    "    first_char.add(sentence[18])\n",
    "    first_char.add(sentence[26])\n",
    "    for char in sentence:\n",
    "        if char not in word_dict:\n",
    "            word_dict.add(char)\n",
    "word_dict.add('eof')\n",
    "word2indx = {word:idx for idx,word in enumerate(sorted(word_dict))}\n",
    "indx2word = {idx:word for idx,word in enumerate(sorted(word_dict))}\n",
    "print('vocab_size:', len(word2indx))\n",
    "print('古诗样本数:', len(raw_data))\n",
    "\n",
    "def make_one_case(sentence) -> (torch.tensor, torch.tensor):\n",
    "    sentence = list(sentence)\n",
    "    sentence.append('eof')\n",
    "    length = len(sentence)\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(1, length):\n",
    "        pre = sentence[i-1]\n",
    "        nex = sentence[i]\n",
    "        inputs.append(word2indx[pre])\n",
    "        targets.append(word2indx[nex])\n",
    "    return torch.tensor(inputs, dtype=torch.long), torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, hidden_size=self.hidden_size)\n",
    "        # self.lstm2 = nn.LSTM(input_size=self.hidden_size, hidden_size=self.hidden_size) # 双层LSTM\n",
    "        self.fc1 = nn.Linear(self.hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, sentence, hidden):\n",
    "        seq_length = sentence.size()[0]\n",
    "        embeds = self.embed(sentence).view(seq_length, 1, -1) # 通过嵌入层之后reshape成(time_step, batch, embedding_size)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        # lstm_out, hidden = self.lstm2(lstm_out.view(seq_length, 1, -1), self.init_hidden())\n",
    "        output = F.relu(self.fc1(lstm_out.view(seq_length, -1)))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1,1,self.hidden_size),\n",
    "                torch.zeros(1,1,self.hidden_size))\n",
    "    \n",
    "# hyperparameters\n",
    "VOCAB_SIZE = len(word2indx)\n",
    "HIDDEN_SIZE = 128\n",
    "EMBEDDING_SIZE = 64\n",
    "\n",
    "\n",
    "model = Net(VOCAB_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE)\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "train = True\n",
    "if os.path.exists('params.pkl'):\n",
    "    model.load_state_dict(torch.load('params.pkl'))\n",
    "    train = False\n",
    "if train == False:\n",
    "    ans = input(r'是否强制训练(Y/N):')\n",
    "    if ans == 'Y' or ans == 'y':\n",
    "        train = True\n",
    "\n",
    "\n",
    "# SGD batch_size=1\n",
    "if train:\n",
    "    print('train start')\n",
    "    epoch = 350\n",
    "    batch_size = len(raw_data)\n",
    "    Loss = []\n",
    "    start_time = time.time()\n",
    "    for i in range(epoch):\n",
    "        _loss = 0\n",
    "        indxs = list(range(batch_size))\n",
    "        random.shuffle(indxs)\n",
    "        for j in indxs:\n",
    "            inputs, targets = make_one_case(raw_data[j])\n",
    "            optimizer.zero_grad()\n",
    "            hidden = model.init_hidden()\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            _loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        _loss /= batch_size\n",
    "        if i % 25 == 0:\n",
    "            print('epoch:{}, loss:{:.2f}'.format(i, _loss))\n",
    "        Loss.append(_loss)\n",
    "    end_time = time.time()\n",
    "    print('train done, cost{}s'.format(end_time-start_time))\n",
    "    torch.save(model.state_dict(), 'params.pkl')\n",
    "    plt.plot(range(len(Loss)), Loss, label='Loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def sample(startword, max_len=15) -> str:\n",
    "    if startword not in word_dict:\n",
    "        return 'null'\n",
    "    inputs = torch.tensor([word2indx[startword]], dtype=torch.long)\n",
    "    output_poetry = startword\n",
    "    hidden = model.init_hidden()\n",
    "    for i in range(max_len):\n",
    "        outputs,hidden = model(inputs, hidden)\n",
    "        topv, topi = outputs.data.topk(1)\n",
    "        print(topi)\n",
    "        w = topi[0][0].item()\n",
    "        word = indx2word[w]\n",
    "        if word == 'eof':\n",
    "            break\n",
    "        else:\n",
    "            output_poetry += word\n",
    "        inputs = torch.tensor([w], dtype=torch.long)\n",
    "    return output_poetry\n",
    "\n",
    "nums = 4\n",
    "for i in range(nums):\n",
    "    word = random.sample(list(word_dict), 1)[0]\n",
    "    print(sample(startword=word, max_len=15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 135842 sentence pairs\n",
      "Trimmed to 10853 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 2925\n",
      "fra 4489\n",
      "loading done..\n",
      "源语言: i m a normal guy .\n",
      "目标语言: je suis un gars normal .\n",
      "tensor([[ 6]])\n",
      "tensor([[ 11]])\n",
      "tensor([[ 66]])\n",
      "tensor([[ 647]])\n",
      "tensor([[ 66]])\n",
      "tensor([[ 5]])\n",
      "tensor([[ 1]])\n",
      "翻译结果: je suis un peu un . <EOS>\n",
      "\n",
      "\n",
      "源语言: you are difficult and incorrigible .\n",
      "目标语言: tu es difficile et incorrigible .\n",
      "tensor([[ 211]])\n",
      "tensor([[ 212]])\n",
      "tensor([[ 66]])\n",
      "tensor([[ 62]])\n",
      "tensor([[ 5]])\n",
      "tensor([[ 5]])\n",
      "tensor([[ 1]])\n",
      "翻译结果: tu es un bon . . <EOS>\n",
      "\n",
      "\n",
      "源语言: i m not proud of it .\n",
      "目标语言: je n en suis pas fiere .\n",
      "tensor([[ 6]])\n",
      "tensor([[ 298]])\n",
      "tensor([[ 11]])\n",
      "tensor([[ 247]])\n",
      "tensor([[ 669]])\n",
      "tensor([[ 5]])\n",
      "tensor([[ 1]])\n",
      "翻译结果: je ne suis pas peur . <EOS>\n",
      "\n",
      "\n",
      "源语言: you re nice .\n",
      "目标语言: vous etes sympa .\n",
      "tensor([[ 211]])\n",
      "tensor([[ 212]])\n",
      "tensor([[ 594]])\n",
      "tensor([[ 5]])\n",
      "tensor([[ 1]])\n",
      "翻译结果: tu es matinal . <EOS>\n",
      "\n",
      "\n",
      "源语言: he s annoying .\n",
      "目标语言: il est embetant .\n",
      "tensor([[ 24]])\n",
      "tensor([[ 25]])\n",
      "tensor([[ 14]])\n",
      "tensor([[ 5]])\n",
      "tensor([[ 1]])\n",
      "翻译结果: il est en . <EOS>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    seq2seq : machine translation\n",
    "    english -> french\n",
    "'''\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'fra', False)\n",
    "random.choice(pairs)\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input为单个单词\n",
    "        embeds = self.embedding(input).view(1,1,-1)\n",
    "        output = embeds\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size)\n",
    "    \n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    '''\n",
    "        未使用Attention机制的Decoder\n",
    "    '''\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input为单个单词\n",
    "        embeds = self.embedding(input).view(1,1,-1)\n",
    "        output = F.relu(embeds)\n",
    "        output,hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size)\n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "    \n",
    "def indexsFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split()]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexs = indexsFromSentence(lang, sentence)\n",
    "    indexs.append(EOS_token)\n",
    "    return torch.tensor(indexs, dtype=torch.long).view(-1, 1)\n",
    "\n",
    "def tensorFrompair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    output_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, output_tensor)\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    plt.plot(points)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, critirion, maxlength = MAX_LENGTH):\n",
    "    loss = 0\n",
    "    encoder_optim.zero_grad()\n",
    "    decoder_optim.zero_grad()\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    input_length = input_tensor.size()[0]\n",
    "    target_length = target_tensor.size()[0]\n",
    "    \n",
    "    encoder_outputs = torch.zeros(maxlength, encoder.hidden_size)\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] += encoder_output[0,0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], dtype=torch.long)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random()<teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += critirion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += critirion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    encoder_optim.step()\n",
    "    decoder_optim.step()\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=200, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    print('train start')\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorFrompair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    # showPlot(plot_losses)\n",
    "    \n",
    "def evaluate(encoder, decoder, sentence, maxlength=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(maxlength, encoder.hidden_size)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0,0]\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.tensor([[SOS_token]], dtype=torch.long)\n",
    "        decoded_words = []\n",
    "        for di in range(maxlength):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words\n",
    "\n",
    "def evaluate_random(encoder, decoder, nums=5):\n",
    "    for _ in range(nums):\n",
    "        pair = random.choice(pairs)\n",
    "        print('源语言:', pair[0])\n",
    "        print('目标语言:', pair[1])\n",
    "        pred = evaluate(encoder, decoder, pair[0])\n",
    "        pred = ' '.join(pred)\n",
    "        print('翻译结果:', pred)\n",
    "        print('\\n')\n",
    "    \n",
    "# hyperparams\n",
    "HIDDEN_SIZE = 128\n",
    "epochs = 30000\n",
    "\n",
    "encoder = EncoderRNN(vocab_size=input_lang.n_words, hidden_size=HIDDEN_SIZE)\n",
    "decoder = AttnDecoderRNN(HIDDEN_SIZE, output_lang.n_words)\n",
    "\n",
    "need_train = True\n",
    "if os.path.exists('encoder.pkl') and os.path.exists('decoder.pkl'):\n",
    "    encoder.load_state_dict(torch.load('encoder.pkl'))\n",
    "    decoder.load_state_dict(torch.load('decoder.pkl'))\n",
    "    print('loading done..')\n",
    "    need_train = False\n",
    "\n",
    "if need_train:\n",
    "    trainIters(encoder, decoder, epochs, print_every=1000)\n",
    "    torch.save(encoder.state_dict(), 'encoder.pkl')\n",
    "    torch.save(decoder.state_dict(), 'decoder.pkl')\n",
    "\n",
    "evaluate_random(encoder, decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-17T15:25:52.328Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\drsy9\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.059 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 20085 sentence pairs\n",
      "Trimmed to 7337 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3073\n",
      "chn 5924\n",
      "train start\n",
      "1m 23s (- 57m 42s) (2000 2%) 4.6806\n",
      "2m 34s (- 52m 4s) (4000 4%) 4.3661\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    seq2seq : machine translation\n",
    "    english -> chinese\n",
    "'''\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import jieba\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def cut_chinese_sentence(sentence):\n",
    "    return ' '.join(jieba.cut(sentence))\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(l.split('\\t')[0]), cut_chinese_sentence(l.split('\\t')[1])] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "MAX_LENGTH = 12\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s\",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \",\n",
    "    \"what \", \"why \",\n",
    "    \"when \", \"where \",\n",
    "    \"i\",\n",
    "    \"it\", \"do\",\n",
    ")\n",
    "\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[0].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepareData('eng', 'chn', False)\n",
    "random.choice(pairs)\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input为单个单词\n",
    "        embeds = self.embedding(input).view(1,1,-1)\n",
    "        output = embeds\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,1,self.hidden_size, device=device)\n",
    "    \n",
    "    \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output.view(1, -1)), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
    "    \n",
    "def indexsFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split()]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexs = indexsFromSentence(lang, sentence)\n",
    "    indexs.append(EOS_token)\n",
    "    return torch.tensor(indexs, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorFrompair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    output_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, output_tensor)\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optim, decoder_optim, critirion, maxlength = MAX_LENGTH):\n",
    "    loss = 0\n",
    "    encoder_optim.zero_grad()\n",
    "    decoder_optim.zero_grad()\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    input_length = input_tensor.size()[0]\n",
    "    target_length = target_tensor.size()[0]\n",
    "    \n",
    "    encoder_outputs = torch.zeros(maxlength, encoder.hidden_size, device=device)\n",
    "    \n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] += encoder_output[0,0]\n",
    "    \n",
    "    decoder_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    use_teacher_forcing = True if random.random()<teacher_forcing_ratio else False\n",
    "    if use_teacher_forcing:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += critirion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]\n",
    "            \n",
    "    else:\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "            loss += critirion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "    loss.backward()\n",
    "    encoder_optim.step()\n",
    "    decoder_optim.step()\n",
    "    return loss.item() / target_length\n",
    "\n",
    "def trainIters(encoder, decoder, n_iters, print_every=200, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    print('train start')\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorFrompair(random.choice(pairs))\n",
    "                      for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "\n",
    "        loss = train(input_tensor, target_tensor, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    # showPlot(plot_losses)\n",
    "    \n",
    "def evaluate(encoder, decoder, sentence, maxlength=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.init_hidden()\n",
    "        \n",
    "        encoder_outputs = torch.zeros(maxlength, encoder.hidden_size, device=device)\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0,0]\n",
    "        \n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_input = torch.tensor([[SOS_token]], dtype=torch.long, device=device)\n",
    "        decoded_words = []\n",
    "        for di in range(maxlength):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "        return decoded_words\n",
    "\n",
    "def evaluate_random(encoder, decoder, nums=5):\n",
    "    random.seed(42)\n",
    "    for _ in range(nums):\n",
    "        pair = random.choice(pairs)\n",
    "        print('源语言:', pair[0])\n",
    "        print('目标语言:', ''.join(pair[1].split()))\n",
    "        pred = evaluate(encoder, decoder, pair[0])\n",
    "        pred = ''.join(pred)\n",
    "        print('翻译结果:', pred)\n",
    "        print('\\n')\n",
    "    \n",
    "def evaluate_input(encoder, decoder):\n",
    "    sent = input(\"请输入一句英文：\")\n",
    "    pred = evaluate(encoder, decoder, sent)\n",
    "    print(\"翻译结果：\", ''.join(pred))\n",
    "    \n",
    "    \n",
    "# hyperparams\n",
    "HIDDEN_SIZE = 256\n",
    "epochs = 85000\n",
    "\n",
    "encoder = EncoderRNN(vocab_size=input_lang.n_words, hidden_size=HIDDEN_SIZE).to(device)\n",
    "decoder = AttnDecoderRNN(HIDDEN_SIZE, output_lang.n_words).to(device)\n",
    "\n",
    "need_train = True\n",
    "# if os.path.exists('encoder2.pkl') and os.path.exists('decoder2.pkl'):\n",
    "#     encoder.load_state_dict(torch.load('encoder2.pkl'))\n",
    "#     decoder.load_state_dict(torch.load('decoder2.pkl'))\n",
    "#     print('loading done..')\n",
    "#     need_train = False\n",
    "\n",
    "if need_train:\n",
    "    trainIters(encoder, decoder, epochs, print_every=2000, plot_every=2000)\n",
    "    torch.save(encoder.state_dict(), 'encoder2.pkl')\n",
    "    torch.save(decoder.state_dict(), 'decoder2.pkl')\n",
    "\n",
    "evaluate_random(encoder, decoder)\n",
    "evaluate_input(encoder, decoder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "notify_time": "5",
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
